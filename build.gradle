buildscript {
    repositories { mavenCentral() }
    dependencies { classpath 'org.ajoberstar:gradle-git:0.6.3' }
}
apply plugin: 'java'
apply plugin: 'github-pages'
ext {
    privateRepo = 'http://115.29.40.120:8181/nexus/content/groups/ailk'
    privateDeployRepo = 'http://115.29.40.120:8181/nexus/content/repositories/ailk-releases'
    deploy_username = System.getProperty('DEPLOY_USERNAME')
    deploy_password = System.getProperty('DEPLOY_PASSWORD')
    oc_version = 'och3.2.0'
    zookeeper_version = '3.4.5-cdh5.0.1'
    hadoop_common_version = '2.3.0-cdh5.0.1'
    hbase_version = '0.96.1.1-cdh5.0.1' 
    hive_version = '0.12.0-cdh5.0.1'
    amplab_hive_version = '0.11.0-shark-0.9.1'
    amplab_shark_version = '0.9.2'
    apache_spark_version = "1.0.0"
}

repositories {
    maven {
        url privateRepo
    }
}
configurations {
    auditor_plugin
}
dependencies {
}

def getFromMaven(src, destDir) {
    s = src.split("[:@]")
    srcUrl = "http://10.1.253.223:8081/nexus/service/local/artifact/maven/redirect?r=ailk&g=${s[0]}&a=${s[1]}&v=${s[2]}&e=${s[3]}"
    ant.echo("downloading:$srcUrl to:$destDir/${s[1]}-${s[2]}.${s[3]}")
    ant.mkdir(dir: destDir)
    ant.get(src: srcUrl, dest: "$destDir/${s[1]}-${s[2]}.${s[3]}", verbose: true)
}

task zookeeper() {
    doLast() {
        project.exec {
            workingDir 'zookeeper'
            commandLine 'ant', "-Dversion=$zookeeper_version", "-Divy.url=$privateRepo/org/apache/ivy/ivy", "clean"
        }
        project.exec {
            workingDir 'zookeeper'
            commandLine 'ant', "-Dversion=$zookeeper_version", "-Divy.url=$privateRepo/org/apache/ivy/ivy", 'tar'
        }
        ant.delete(dir: 'zookeeper/build/och')
        ant.mkdir(dir: 'zookeeper/build/och')
        project.exec {
            workingDir 'zookeeper/build'
            commandLine "tar -xvzf  zookeeper-${zookeeper_version}.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "zookeeper/build/och/zookeeper-${zookeeper_version}", tofile: "zookeeper/build/och/zookeeper-${zookeeper_version}-${oc_version}")
        project.exec {
            workingDir 'zookeeper/build/och'
            commandLine "tar -cvzf zookeeper-${zookeeper_version}-${oc_version}.tar.gz zookeeper-${zookeeper_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "zookeeper/build/och/zookeeper-${zookeeper_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}
task hadoop_common() {
    doLast() {
        project.exec {
            workingDir 'hadoop-common'
            commandLine 'mvn', "-Pdist,native", "-DskipTests", "-Dmaven.javadoc.skip=true", "-Dprotoc.path=" + System.getProperty('PROTOBUF_HOME', "/home/jenkins/app/protobuf-2.5.0") + "/bin/protoc", "-Dtar", "-Drequire.snappy=true", "-Dsnappy.prefix=" + System.getProperty('SNAPPY_HOME', "/home/jenkins/app/snappy-1.1.2"), "clean", "install"
        }
        ant.delete(dir: 'hadoop-common/hadoop-dist/target/och')
        ant.mkdir(dir: 'hadoop-common/hadoop-dist/target/och')
        project.exec {
            workingDir 'hadoop-common/hadoop-dist/target'
            commandLine "tar -xvzf  hadoop-${hadoop_common_version}.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "hadoop-common/hadoop-dist/target/och/hadoop-${hadoop_common_version}", tofile: "hadoop-common/hadoop-dist/target/och/hadoop-${hadoop_common_version}-${oc_version}")
        project.exec {
            workingDir 'hadoop-common/hadoop-dist/target/och'
            commandLine "tar -cvzf hadoop-${hadoop_common_version}-${oc_version}.tar.gz hadoop-${hadoop_common_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "hadoop-common/hadoop-dist/target/och/hadoop-${hadoop_common_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}
task hbase() {
    doLast() {
        project.exec {
            workingDir 'hbase'
            commandLine "mvn -DskipTests -Pcdh5 -Dmaven.javadoc.skip=false -Dhadoop.profile=2.0 -Dcdh.hadoop.version=$hadoop_common_version clean install assembly:single".split().toList()
        }
        ant.delete(dir: 'hbase/hbase-assembly/target/och')
        ant.mkdir(dir: 'hbase/hbase-assembly/target/och')
        project.exec {
            workingDir 'hbase/hbase-assembly/target'
            commandLine "tar -xvzf  hbase-${hbase_version}-bin.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "hbase/hbase-assembly/target/och/hbase-${hbase_version}", tofile: "hbase/hbase-assembly/target/och/hbase-${hbase_version}-${oc_version}")
        project.exec {
            workingDir 'hbase/hbase-assembly/target/och'
            commandLine "tar -cvzf hbase-${hbase_version}-${oc_version}.tar.gz hbase-${hbase_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "hbase/hbase-assembly/target/och/hbase-${hbase_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}

task hive() {
    doLast() {
        project.exec {
            workingDir 'hive'
            commandLine 'mvn', "-T", "2C", "-Dmaven.javadoc.skip=true", "-Dmaven.test.skip=true", "-Phadoop-2,dist", "-Dcdh.hadoop.version=$hadoop_common_version", "clean", 'install'
        }

        ant.delete(dir: 'hive/packaging/target/och')
        ant.mkdir(dir: 'hive/packaging/target/och')
        project.exec {
            workingDir 'hive/packaging/target'
            commandLine "tar -xvzf  apache-hive-${hive_version}-bin.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "hive/packaging/target/och/apache-hive-${hive_version}-bin", tofile: "hive/packaging/target/och/hive-${hive_version}-${oc_version}")
        project.exec {
            workingDir 'hive/packaging/target/och'
            commandLine "tar -cvzf hive-${hive_version}-${oc_version}.tar.gz hive-${hive_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "hive/packaging/target/och/hive-${hive_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}

task apache_spark() {
    doLast() {
//        project.exec {
//            workingDir 'spark'
//            commandLine './make-distribution.sh'
//        }
        project.exec {
            workingDir 'apache-spark'
//            commandLine "mvn -Dcdh.hadoop.version=$hadoop_common_version -Dcdh.protobuf.version=2.5.0 -Dcdh.hbase.version=$hbase_version clean install"
            commandLine "mvn -e -Pyarn -Dhadoop.version=$hadoop_common_version -Dyarn.version=$hadoop_common_version -DskipTests clean install".split().toList()
            environment MAVEN_OPTS: "-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
            environment SCALA_HOME: "/home/jenkins/app/scala-2.10.3"
        }
        ant.copy(file: "apache-spark/assembly/target/scala-2.10/spark-assembly-$apache_spark_version-hadoop${hadoop_common_version}.jar", tofile: "build/ochadoop/spark-assembly_$apache_spark_version-hadoop${hadoop_common_version}-${oc_version}.jar")
    }
}
task amplab_hive() {
    doLast() {
        String contents = new File('amplab-hive/ivy/ivysettings.xml').getText('UTF-8')
        contents = contents.replaceAll('http://repo1.maven.org/maven2/', "$privateRepo")
        new File('amplab-hive/ivy/ivysettings.xml').write(contents, 'UTF-8')

        contents = new File('amplab-hive/build.xml').getText('UTF-8')
        contents = contents.replaceAll('<target name="javadoc"', '<target name="javadoc" unless="skip.javadoc"')
        contents = contents.replaceAll('(<remoteRepository.*)/>', '$1><authentication username="\\${mvn.deploy.username}" password="\\${mvn.deploy.password}" /></remoteRepository>')
        // skip hcatalog
        contents = contents.replaceAll('<ant dir="hcatalog"[^/]*/[^/]*/[^/]*/ant>', '')
        new File('amplab-hive/build.skip.javadoc.xml').write(contents, 'UTF-8')


        project.exec {
            workingDir 'amplab-hive'
            commandLine "ant package maven-build maven-publish -buildfile build.skip.javadoc.xml -Dskip.javadoc=true -Drepo.maven.org=$privateRepo -Divy.mvn.repo=$privateRepo -Dmvnrepo=$privateRepo -Dmvn.deploy.url=$privateDeployRepo -Dmvn.deploy.id=ailk -Dversion=$amplab_hive_version-$oc_version -Dmvn.deploy.username=$deploy_username -Dmvn.deploy.password=$deploy_password -Dhive.version=0.11.0-shark-0.9.1-och3.0.0-SNAPSHOT -Dprotobuf.version=2.5.0".split().toList()
        }
    }
}
task amplab_shark() {
    doLast() {
        project.exec {
            workingDir 'amplab-shark'
            commandLine "git checkout -f project/SharkBuild.scala".split().toList()
        }
        String contents = new File('amplab-shark/project/SharkBuild.scala').getText('UTF-8')
        contents = contents.replaceAll('SHARK_VERSION = "1.0.0-PREVIEW-SNAPSHOT"', "SHARK_VERSION = \"$amplab_shark_version-${oc_version}\"")
        //TODO: shark 0.11.0-shark-0.9.1 not publish
        contents = contents.replaceAll("HIVE_VERSION = \"$amplab_hive_version-SNAPSHOT\"", "HIVE_VERSION = \"$amplab_hive_version-${oc_version}\"")
        contents = contents.replaceAll('SPARK_VERSION = "1.0.0-SNAPSHOT"', "SPARK_VERSION = \"${apache_spark_version}\"")
        contents = contents.replaceAll('DEFAULT_HADOOP_VERSION = "1.0.4"', "DEFAULT_HADOOP_VERSION = \"$hadoop_common_version\"")
        contents = contents.replaceAll('resolvers \\+\\+= Seq\\(', "resolvers ++= Seq(\n      \"ailk Repository\" at \"$privateRepo\",")
        new File('amplab-shark/project/SharkBuild.scala').write(contents, 'UTF-8')

        project.exec {
            workingDir 'amplab-shark'
            commandLine "sbt/sbt clean package assembly".split().toList()
        }
        ant.delete(dir: "shark-$amplab_shark_version-${oc_version}")

        project.exec {
            workingDir './'
            commandLine "cp -r amplab-shark shark-$amplab_shark_version-${oc_version}".split().toList()
        }
        //ant.delete(file: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars/org.spark-project.protobuf/protobuf-java/protobuf-java-2.4.1-shaded.jar")
//        ant.delete(file: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars/edu.berkeley.cs.shark/hive-exec/hive-exec-0.11.0-shark-0.9.1-och3.0.0-SNAPSHOT.jar")
        ant.delete{
         	 fileset(dir: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars", excludes: 'org.datanucleus')
         }
        ant.mkdir(dir:  "shark-$amplab_shark_version-${oc_version}/lib_managed/jars_bak")
        ant.move(file: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars/org.datanucleus" tofile: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars_bak/")
        ant.delete(dir: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars")
         ant.move(file: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars_bak" tofile: "shark-$amplab_shark_version-${oc_version}/lib_managed/jars")
    	ant.delete(dir: "shark-$amplab_shark_version-${oc_version}/lib_managed/orbits")
        ant.delete(dir: "shark-$amplab_shark_version-${oc_version}/lib_managed/bundles")
       
        ant.copy(file: "shark-$amplab_shark_version-${oc_version}/conf/shark-env.sh.template", tofile: "shark-$amplab_shark_version-${oc_version}/conf/shark-env.sh")

        contents = new File("shark-$amplab_shark_version-${oc_version}/run").getText('UTF-8')
        contents = contents.replaceAll('(exec \\$RUNNER \\$EXTRA_ARGS .*)', 'CLASSPATH+=":\\$SPARK_ASSEMBLY_JAR"\nCLASSPATH+=":\\$SHARK_ASSEMBLY_JAR"\n$1')
        new File("shark-$amplab_shark_version-${oc_version}/run").write(contents, 'UTF-8')

        project.exec {
            workingDir './'
            commandLine "tar -cvzf shark-$amplab_shark_version-${oc_version}.tar.gz shark-$amplab_shark_version-${oc_version}".split().toList()
        }
        ant.move(file: "shark-$amplab_shark_version-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}

task all(dependsOn: [zookeeper, hadoop_common, hbase, hive, apache_spark, amplab_hive, amplab_shark]) {
    doLast {
        ant.mkdir(dir: "build/tar/ochadoop-${oc_version}")
        ant.move(file: "build/ochadoop", tofile: "build/tar/ochadoop-${oc_version}/ochadoop-${oc_version}")
        ant.tar(basedir: "build/tar/ochadoop-${oc_version}", destfile: "build/tar/ochadoop-${oc_version}.tar")
        ant.gzip(src: "build/tar/ochadoop-${oc_version}.tar", destfile: "build/tar/ochadoop-${oc_version}.tar.gz")
    }
}


