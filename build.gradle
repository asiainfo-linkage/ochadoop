buildscript {
    repositories { mavenCentral() }
    dependencies { classpath 'org.ajoberstar:gradle-git:0.6.3' }
}
apply plugin: 'java'
apply plugin: 'github-pages'
ext {
    privateRepo = 'http://103.31.200.123:8081/nexus/content/groups/ailk'
    privateDeployRepo = 'http://103.31.200.123:8081/nexus/content/repositories/ailk-snapshots'
    forrest_home = System.getProperty('FORREST_HOME', "/home/cloudetl/apache-forrest-0.9")
    oc_version = 'och3.0.0-SNAPSHOT'
    zookeeper_version = '3.4.5-cdh5.0.0-beta-2'
    hadoop_common_version = '2.2.0-cdh5.0.0-beta-2'
    hbase_version = '0.96.1.1-cdh5.0.0-beta-2'
    hive_version = '0.12.0-cdh5.0.0-beta-2'
    hadoop_common_mr1_version = '2.0.0-mr1-cdh4.2.1'
    apache_hbase_version = '0.94.12-security'
    apache_hive_version = '0.12.0'
    apache_hive_legacy_version = '0.11.0'
    hadoop_lzo_version = '0.4.19'
    auditor_version = 'och2.0.0-beta'
    apache_hive_extension_version = '0.12.0-och2.0.0-beta'
    apache_hive_extension_legacy_version = '0.11.0-och2.0.0-beta'
    apache_spark_version = "2.10-0.9.1"
}

repositories {
    maven {
        url privateRepo
    }
}
configurations {
    auditor_plugin
}
dependencies {
    auditor_plugin "com.ailk.oci.auditor:auditor-plugin:$auditor_version"
}

def getFromMaven(src, destDir) {
    s = src.split("[:@]")
    srcUrl = "http://10.1.253.223:8081/nexus/service/local/artifact/maven/redirect?r=ailk&g=${s[0]}&a=${s[1]}&v=${s[2]}&e=${s[3]}"
    ant.echo("downloading:$srcUrl to:$destDir/${s[1]}-${s[2]}.${s[3]}")
    ant.mkdir(dir: destDir)
    ant.get(src: srcUrl, dest: "$destDir/${s[1]}-${s[2]}.${s[3]}", verbose: true)
}


task auditor() {
    doLast {
        getFromMaven("com.ailk.oci.auditor:auditor-server:$auditor_version@jar", "$buildDir/ochadoop")
    }
}

task zookeeper() {
    doLast() {
        project.exec {
            workingDir 'zookeeper'
            commandLine 'ant', "-Dversion=$zookeeper_version", "-Divy.url=$privateRepo/org/apache/ivy/ivy", "clean"
        }
        project.exec {
            workingDir 'zookeeper'
            commandLine 'ant', "-Dversion=$zookeeper_version", "-Divy.url=$privateRepo/org/apache/ivy/ivy", 'tar'
        }
        ant.delete(dir: 'zookeeper/build/och')
        ant.mkdir(dir: 'zookeeper/build/och')
        project.exec {
            workingDir 'zookeeper/build'
            commandLine "tar -xvzf  zookeeper-${zookeeper_version}.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "zookeeper/build/och/zookeeper-${zookeeper_version}", tofile: "zookeeper/build/och/zookeeper-${zookeeper_version}-${oc_version}")
        project.exec {
            workingDir 'zookeeper/build/och'
            commandLine "tar -cvzf zookeeper-${zookeeper_version}-${oc_version}.tar.gz zookeeper-${zookeeper_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "zookeeper/build/och/zookeeper-${zookeeper_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}
task hadoop_common() {
    doLast() {
        project.exec {
            workingDir 'hadoop-common'
            commandLine 'mvn', "-Pdist,native", "-DskipTests", "-Dmaven.javadoc.skip=true", "-Dprotoc.path=" + System.getProperty('PROTOBUF_HOME', "/home/jenkins/app/protobuf-2.5.0") + "/bin/protoc", "-Dtar", "-Drequire.snappy=true", "-Dsnappy.prefix=" + System.getProperty('SNAPPY_HOME', "/home/jenkins/app/snappy-1.1.2"), "clean", "install"
        }
        ant.delete(dir: 'hadoop-common/hadoop-dist/target/och')
        ant.mkdir(dir: 'hadoop-common/hadoop-dist/target/och')
        project.exec {
            workingDir 'hadoop-common/hadoop-dist/target'
            commandLine "tar -xvzf  hadoop-${hadoop_common_version}.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "hadoop-common/hadoop-dist/target/och/hadoop-${hadoop_common_version}", tofile: "hadoop-common/hadoop-dist/target/och/hadoop-${hadoop_common_version}-${oc_version}")
        project.exec {
            workingDir 'hadoop-common/hadoop-dist/target/och'
            commandLine "tar -cvzf hadoop-${hadoop_common_version}-${oc_version}.tar.gz hadoop-${hadoop_common_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "hadoop-common/hadoop-dist/target/och/hadoop-${hadoop_common_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}
task hbase() {
    doLast() {
        project.exec {
            workingDir 'hbase'
            commandLine "mvn -DskipTests -Pcdh5 -Dmaven.javadoc.skip=false -Dhadoop.profile=2.0 -Dcdh.hadoop.version=$hadoop_common_version clean install assembly:single".split().toList()
        }
        ant.delete(dir: 'hbase/hbase-assembly/target/och')
        ant.mkdir(dir: 'hbase/hbase-assembly/target/och')
        project.exec {
            workingDir 'hbase/hbase-assembly/target'
            commandLine "tar -xvzf  hbase-${hbase_version}-bin.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "hbase/hbase-assembly/target/och/hbase-${hbase_version}", tofile: "hbase/hbase-assembly/target/och/hbase-${hbase_version}-${oc_version}")
        project.exec {
            workingDir 'hbase/hbase-assembly/target/och'
            commandLine "tar -cvzf hbase-${hbase_version}-${oc_version}.tar.gz hbase-${hbase_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "hbase/hbase-assembly/target/och/hbase-${hbase_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}

task hive() {
    doLast() {
        project.exec {
            workingDir 'hive'
            commandLine 'mvn', "-T", "2C", "-Dmaven.javadoc.skip=true", "-Dmaven.test.skip=true", "-Phadoop-2,dist", "-Dcdh.hadoop.version=$hadoop_common_version", "clean", 'install'
        }

        ant.delete(dir: 'hive/packaging/target/och')
        ant.mkdir(dir: 'hive/packaging/target/och')
        project.exec {
            workingDir 'hive/packaging/target'
            commandLine "tar -xvzf  apache-hive-${hive_version}-bin.tar.gz -C ./och".split().toList()
        }
        ant.move(file: "hive/packaging/target/och/apache-hive-${hive_version}-bin", tofile: "hive/packaging/target/och/hive-${hive_version}-${oc_version}")
        project.exec {
            workingDir 'hive/packaging/target/och'
            commandLine "tar -cvzf hive-${hive_version}-${oc_version}.tar.gz hive-${hive_version}-${oc_version}".split().toList()
        }
        ant.mkdir(dir: "build/ochadoop")
        ant.move(file: "hive/packaging/target/och/hive-${hive_version}-${oc_version}.tar.gz", todir: "build/ochadoop")
    }
}

task apache_spark() {
    doLast() {
//        project.exec {
//            workingDir 'spark'
//            commandLine './make-distribution.sh'
//        }
        project.exec {
            workingDir 'apache-spark'
//            commandLine "mvn -Dcdh.hadoop.version=$hadoop_common_version -Dcdh.protobuf.version=2.5.0 -Dcdh.hbase.version=$hbase_version clean install"
            commandLine "mvn -Pyarn -Dhadoop.version=$hadoop_common_version -Dyarn.version=$hadoop_common_version -DskipTests clean install".split().toList()
            environment MAVEN_OPTS: "-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
            environment SCALA_HOME: "/home/jenkins/app/scala-2.10.3"
        }
        ant.copy(file: "apache-spark/assembly/target/scala-2.10/spark-assembly_$apache_spark_version-hadoop${hadoop_common_version}.jar", tofile: "build/ochadoop/spark-assembly_$apache_spark_version-hadoop${hadoop_common_version}-${oc_version}.jar")
    }
}
task amplab_hive() {
    doLast() {
        String contents = new File('amplab-hive/ivy/ivysettings.xml').getText('UTF-8')
        contents = contents.replaceAll('http://repo1.maven.org/maven2/', "$privateRepo")
        new File('amplab-hive/ivy/ivysettings.xml').write(contents, 'UTF-8')

        contents = new File('amplab-hive/build.xml').getText('UTF-8')
        contents = contents.replaceAll('<target name="javadoc"', '<target name="javadoc" unless="skip.javadoc"')
        new File('amplab-hive/build.skip.javadoc.xml').write(contents, 'UTF-8')

        project.exec {
            workingDir 'amplab-hive'
            commandLine "ant maven-build maven-publish -buildfile build.skip.javadoc.xml -Dskip.javadoc=true -Drepo.maven.org=$privateRepo -Divy.mvn.repo=$privateRepo -Dmvnrepo=$privateRepo".split().toList()
        }
    }
}
task amplab_shark() {
    doLast() {
        project.exec {
            workingDir 'amplab-shark'
            commandLine "git checkout -f project/SharkBuild.scala".split().toList()
        }
        String contents = new File('amplab-shark/project/SharkBuild.scala').getText('UTF-8')
        contents = contents.replaceAll('SHARK_VERSION = "0.9.1-SNAPSHOT"', 'SHARK_VERSION = "0.9.1-och3.0.0-SNAPSHOT"')
        contents = contents.replaceAll('HIVE_VERSION = "0.11.0-shark-0.9.1-SNAPSHOT"', 'HIVE_VERSION = "0.11.0-shark-0.9.1-SNAPSHOT"')
//TODO: shark 0.11.0-shark-0.9.1 not publish
        contents = contents.replaceAll('SPARK_VERSION = "0.9.0-incubating"', 'SPARK_VERSION = "0.9.0-cdh5.0.0-beta-2"')
        contents = contents.replaceAll('DEFAULT_HADOOP_VERSION = "1.0.4"', 'DEFAULT_HADOOP_VERSION = "2.2.0-cdh5.0.0-beta-2"')
        new File('amplab-shark/project/SharkBuild.scala').write(contents, 'UTF-8')

        project.exec {
            workingDir 'amplab-shark'
            commandLine "sbt/sbt clean package".split().toList()
        }

        project.exec {
            workingDir './'
            commandLine "tar -cvzf amplab-shark.tar.gz amplab-shark".split().toList()
        }
    }
}

task all(dependsOn: [zookeeper, hadoop_common, hbase, hive, apache_spark]) {
    doLast {
        ant.mkdir(dir: "build/tar/ochadoop-${oc_version}")
        ant.move(file: "build/ochadoop", tofile: "build/tar/ochadoop-${oc_version}/ochadoop-${oc_version}")
        ant.tar(basedir: "build/tar/ochadoop-${oc_version}", destfile: "build/tar/ochadoop-${oc_version}.tar")
        ant.gzip(src: "build/tar/ochadoop-${oc_version}.tar", destfile: "build/tar/ochadoop-${oc_version}.tar.gz")
    }
}


